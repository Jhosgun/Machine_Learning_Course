{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6p6bm09EZqhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urPc1MsU4siM"
      },
      "source": [
        "<img src=\"https://gitlab.com/bivl2ab/academico/cursos-uis/ai/ai-uis-student/raw/master/imgs/ML.jpg\" alt=\"Drawing\" style=\"width:1000px;\">\n",
        "\n",
        "\n",
        "\n",
        "# <center> 07.Delving Deeper into the Numbers\n",
        "This lesson is dedicated to exploring the fundamental mathematical concepts behind two key areas of machine learning: linear regression and classification. We'll delve into the core principles that govern these techniques, emphasizing the derivation and application of their respective formulas. Through detailed analysis, we aim to deepen your understanding of how these methods work and their significance in predictive modeling and data analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Simple Linear Regression"
      ],
      "metadata": {
        "id": "1H7Lw2FUO2XF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear regression is a method used to understand the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable, and the other is considered to be a dependent variable. For example, you might use linear regression to understand whether and how the number of hours you study affects your scores on an exam. Here, the number of hours studied is the explanatory variable, and the score is the dependent variable.\n",
        "\n",
        "The purpose of linear regression is to predict the value of the dependent variable based on the values of the explanatory variable. It essentially draws a straight line (hence \"linear\") that best represents the data according to a mathematical criterion called the \"least squares.\""
      ],
      "metadata": {
        "id": "6dL27YbwTCHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Formula"
      ],
      "metadata": {
        "id": "6IB2xBElUJj3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Y=β0\n",
        "​\n",
        " +β1\n",
        "​\n",
        " x+ϵ**\n",
        "\n",
        "- Y is the dependent variable (e.g., exam score).\n",
        "\n",
        "- x is the independent variable (e.g., hours studied).\n",
        "- β0 is the intercept, representing the predicted value of y when x is 0.\n",
        "- β1 is the slope of the line, representing how much y changes for a one-unit change in x.\n",
        "- ϵ is the error term, accounting for the variability in y that cannot be explained by x alone."
      ],
      "metadata": {
        "id": "rLc2HdldUPtq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Formulas for Coefficients"
      ],
      "metadata": {
        "id": "L6ZnjbLZHrlp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The coefficients of the linear regression equation $y = \\beta_0 + \\beta_1 x$ are determined by solving for the slope ($\\beta_1$) and the intercept ($\\beta_0$).\n",
        "\n",
        "**Slope ($\\beta_1$):**\n",
        "The slope of the regression line ($\\beta_1$) indicates how much the dependent variable $y$ is expected to increase when the independent variable $x$ increases by one unit. It is calculated using the formula:\n",
        "\n",
        "$$\\beta_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}$$\n",
        "\n",
        "Here:\n",
        "- $x_i$ and $y_i$ are the individual sample points.\n",
        "- $\\bar{x}$ and $\\bar{y}$ are the mean values of the $x$ and $y$ datasets, respectively.\n",
        "This formula essentially computes a weighted average of the product of deviations of $x$ and $y$ from their respective means.\n",
        "\n",
        "**Intercept ($\\beta_0$):**\n",
        "The intercept ($\\beta_0$) is the predicted value of $y$ when $x$ is 0. It is calculated as:\n",
        "\n",
        "$$\\beta_0 = \\bar{y} - \\beta_1 \\bar{x}$$\n",
        "\n",
        "This formula adjusts the regression line so that the mean of the residuals is zero, ensuring that the line best fits the data according to the least squares criterion.\n",
        "\n"
      ],
      "metadata": {
        "id": "-csKxpDJNN60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Least Squares Method"
      ],
      "metadata": {
        "id": "gZzC64_zCulu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Least Squares Method aims to minimize the sum of these squared residuals across all data points. Mathematically, it is expressed as:\n",
        "$$\\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$$\n",
        "\n",
        "\n",
        "Where\n",
        "SSE stands for the Sum of Squared Errors. By squaring the residuals, we ensure that larger errors are disproportionately penalized, which helps in achieving a more accurate prediction line"
      ],
      "metadata": {
        "id": "QVV2RKvDCwuu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##What are residuals?"
      ],
      "metadata": {
        "id": "yAfFjbL6Y1U6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Capture.PNG](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "H_xh5jjbZ3A1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A residual is the error in the prediction of the dependent variable (\n",
        "y). Specifically, for each data point, the residual is the vertical distance between the actual observed value of the dependent variable and the value predicted by the regression line. If we denote the observed values of\n",
        "y as\n",
        "yi and the predicted values as\n",
        "𝑦𝑖^ , the residual (𝑒𝑖) can be written as:\n",
        "\n",
        "𝑒𝑖\n",
        "=\n",
        "𝑦𝑖\n",
        "−\n",
        "𝑦𝑖^\n",
        "\n"
      ],
      "metadata": {
        "id": "50TvBVtfamNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Analysing The Math In The Code"
      ],
      "metadata": {
        "id": "aAP8-h4DbcfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_simple = sm.add_constant(d['suc'])\n",
        "y = d['angle']"
      ],
      "metadata": {
        "id": "osC1AkTQbfOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- X_simple: This represents the independent variable data ('suc') along with a column of ones added. The column of ones is added as a constant using sm.add_constant, which allows the regression model to include an intercept term (𝛽0).\n",
        "- y: This is the dependent variable data ('angle'). The goal is to model 'angle' as a function of 'suc'."
      ],
      "metadata": {
        "id": "M60WiWXxb1kc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_simple = sm.OLS(y, X_simple).fit()"
      ],
      "metadata": {
        "id": "DODNY7--b54i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- sm.OLS: Creates an Ordinary Least Squares (OLS) regression model. It is called with y (dependent variable) and X_simple (independent variable plus a constant).\n",
        "- fit(): Fits the OLS model to the provided data, finding the coefficient values that minimize the sum of squared residuals between the observed values in y and those predicted by the model."
      ],
      "metadata": {
        "id": "oCFP1D5ccI3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Classification"
      ],
      "metadata": {
        "id": "CJdrmOgZdw9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Naive Gaussian Bayes"
      ],
      "metadata": {
        "id": "vnsnYA9TexK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bayes' Theorem**\n",
        "\n",
        "The Naive Bayes classifier applies Bayes' theorem, which describes the probability of an event based on prior knowledge of conditions that might be related to the event. The theorem is expressed as:\n",
        "\n",
        "$$P(A \\mid B) = \\frac{P(B \\mid A) \\times P(A)}{P(B)}$$\n",
        "\n",
        "Where:\n",
        "- $P(A \\mid B)$ is the posterior probability of class (A, target) given predictor (B, attributes).\n",
        "- $P(B \\mid A)$ is the likelihood, which is the probability of the predictor given the class.\n",
        "- $P(A)$ is the prior probability of the class.\n",
        "- $P(B)$ is the prior probability of the predictor.\n"
      ],
      "metadata": {
        "id": "sY0R53MUm50o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applying to Gaussian Naive Bayes**\n",
        "\n",
        "In Gaussian Naive Bayes, the likelihood of the features is assumed to follow a Gaussian (normal) distribution:\n",
        "\n",
        "$$P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi \\sigma_y^2}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2 \\sigma_y^2}\\right)$$\n",
        "\n",
        "Where:\n",
        "- $x_i$ is a feature.\n",
        "- $y$ is the class.\n",
        "- $\\mu_y$ is the mean of the features for class $y$.\n",
        "- $\\sigma_y^2$ is the variance of the features for class $y$.\n"
      ],
      "metadata": {
        "id": "FaYj8QRZqi7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Working of Gaussian Naive Bayes**\n",
        "\n",
        "**Training Phase:**\n",
        "1. Calculate the mean ($\\mu$) and variance ($\\sigma^2$) of each feature for each class.\n",
        "2. Compute the prior probability for each class based on the frequency of each class in the training data.\n",
        "\n",
        "**Prediction Phase:**\n",
        "1. For a given instance to be classified, calculate the posterior probability for each class based on the Gaussian formula. Multiply these probabilities by the prior probability of the class\n"
      ],
      "metadata": {
        "id": "AtIa97BRu9LJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Decision Tree classifier"
      ],
      "metadata": {
        "id": "m7PArPVnvk4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Decision Trees are a non-parametric supervised learning method used for classification and regression tasks. They are intuitive and easy to visualize but involve complex mathematical foundations for building the tree. Let's explore the mathematical concepts and formulas behind the Decision Tree classifier."
      ],
      "metadata": {
        "id": "yZgyBGPav2qq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Decision Tree: Mathematical Concepts"
      ],
      "metadata": {
        "id": "imLUISqqwoow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Node Splitting Criteria**\n",
        "Decision Trees work by splitting the data into subsets based on feature values. These splits are made at nodes in the tree. Several statistical measures determine where and how to make these splits:\n",
        "\n"
      ],
      "metadata": {
        "id": "vTe598jUwjOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Gini Impurity**\n",
        "Gini impurity is a measure used in the context of decision trees to quantify how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. The Gini impurity is a criterion to minimize the probability of misclassification.\n",
        "\n",
        "**Formula:**\n",
        "$$I_G(p) = 1 - \\sum_{i=1}^k p_i^2$$\n",
        "Where $p_i$ is the probability of an item with label $i$ being chosen and $k$ is the number of classes.\n",
        "\n",
        "**Explanation:**\n",
        "The term $\\sum_{i=1}^k p_i^2$ represents the sum of the squares of the probabilities of each class, which is the probability that a randomly chosen element from the set is correctly labeled. The Gini impurity is then calculated as one minus this sum. This calculation gives us a measure of impurity or mixture in the subset—0 when all elements are of one class (pure), and reaches its maximum when elements are evenly distributed across all classes (highest impurity or disorder).\n"
      ],
      "metadata": {
        "id": "ZyAmHpiOxbRm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Entropy**\n",
        "Entropy, borrowed from information theory, measures the amount of information disorder or uncertainty. In the context of decision trees, it is used as a splitting criterion that aims to maximize the information gain—reducing uncertainty about a sample's classification after a split.\n",
        "\n",
        "**Formula:**\n",
        "$$H(T) = -\\sum_{i=1}^k p_i \\log_2(p_i)$$\n",
        "Where $p_i$ is the proportion of the samples that belong to class $i$.\n",
        "\n",
        "**Explanation:**\n",
        "- If all members of a subset belong to a single class (no uncertainty), the entropy is 0.\n",
        "- If the subset is equally divided among all available classes (maximum uncertainty), entropy is maximized.\n",
        "- Entropy thus provides a measure of the \"purity\" of a collection of examples—if all examples are of the same class, entropy is minimal.\n"
      ],
      "metadata": {
        "id": "YvCpiVWdyCgt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Choosing Between Gini Impurity and Entropy\n",
        "The choice between using Gini impurity and entropy often depends on the specific requirements of the problem and computational considerations:\n",
        "\n",
        "Gini Impurity tends to be faster to compute as it does not require calculating logarithmic functions, which are computationally intensive.\n",
        "Entropy can lead to slightly more balanced trees because it tends to prefer splits that lead to equally sized sets."
      ],
      "metadata": {
        "id": "Z44pTOngyQQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "####**Information Gain:**\n",
        "$$IG(T, a) = H(T) - \\sum_{j=1}^{k} \\frac{|T_j|}{|T|} H(T_j)$$\n",
        "Where $IG(T, a)$ is the information gain of a split $a$, $T$ represents the parent node, and $T_j$ are the subsets of $T$ created by the split $a$, $k$ is the number of splits. Information gain is the decrease in entropy after a dataset is split on an attribute. Constructing a decision tree is all about finding attribute that returns the highest information gain (i.e., the most homogeneous branches).\n",
        "\n",
        "**2. Decision Rule for Splitting Nodes**\n",
        "The decision to split at each node is based on the criterion (like Gini or Entropy) that provides the maximum information gain. The attribute with the highest information gain is chosen to make the decision split at that particular node.\n",
        "\n",
        "**3. Stopping Criteria**\n",
        "Decision trees can keep growing indefinitely if not controlled. To prevent overfitting, several stopping criteria are applied:\n",
        "- Maximum depth of the tree.\n",
        "- Minimum number of samples required to split.\n",
        "- Minimum number of samples a leaf must have.\n",
        "- Maximum number of leaves.\n",
        "- A threshold beneath which a split must not decrease the impurity.\n",
        "\n",
        "**4. Pruning**\n",
        "Pruning is used to reduce the size of a decision tree after it has been built. It involves removing parts of the tree that do not provide power to classify instances. This process can improve the model's generalizability and reduce overfitting.\n"
      ],
      "metadata": {
        "id": "ywru6hq0yHcf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Random Forest classifier (RFC)"
      ],
      "metadata": {
        "id": "NJiVea_gvsH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Random Forest classifier (RFC) is an ensemble learning method for classification (and regression) that operates by constructing a multitude of decision trees at training time and outputting the class that is the majority vote (or average prediction) of the individual trees. Random Forests address some of the key limitations of decision trees, such as overfitting to the training set."
      ],
      "metadata": {
        "id": "djf7OQYr0CXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Mathematical Concepts Behind Random Forest\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7RBGfXns0OfJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Bootstrap Sampling**\n",
        "\n",
        "The bootstrap sampling method used in Random Forest is a statistical technique to generate \"n\" different datasets. Each dataset is created by randomly selecting observations from the original dataset with replacement:\n",
        "$$D_i = \\text{sample}(D, \\text{size} = n, \\text{replace} = \\text{true})$$\n",
        "Where:\n",
        "- $D$ is the original dataset.\n",
        "- $D_i$ is the $i$-th bootstrapped dataset.\n",
        "- $n$ is the number of observations in the original dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "g1VxNrL03XgG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "####**Node Splitting in Decision Trees**\n",
        "\n",
        "Each tree in the Random Forest uses a subset of features (variables) at each split in the training of the tree. This approach is implemented as follows:\n",
        "$$\\text{Feature subset} = \\text{random_sample(Features, max_features)}$$\n",
        "Where:\n",
        "- `max_features` is a hyperparameter that defines the number of features to consider when looking for the best split at each node.\n"
      ],
      "metadata": {
        "id": "cXqq4Qcr3ayF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "####**Information Gain and Gini Impurity**\n",
        "\n",
        "- Each decision tree in a Random Forest uses criteria like Gini impurity or entropy/information gain (as discussed earlier) to decide where to split and how to build the trees.\n",
        "\n",
        "**Gini Impurity for a Split**\n",
        "\n",
        "Used to calculate the quality of a split. For a given node, the Gini impurity is calculated as:\n",
        "$$I_G(t) = 1 - \\sum_{j=1}^c p_j^2$$\n",
        "Where:\n",
        "- $p_j$ is the proportion of the samples at node $t$ that belong to class $j$.\n",
        "- $c$ is the total number of classes.\n",
        "\n",
        "**Information Gain from a Split**\n",
        "\n",
        "Random Forest can also use information gain, which is the difference in impurity from before to after the split. It is calculated as:\n",
        "$$IG(T, a) = H(T) - \\left(\\frac{n_{\\text{left}}}{n} H(T_{\\text{left}}) + \\frac{n_{\\text{right}}}{n} H(T_{\\text{right}})\\right)$$\n",
        "Where:\n",
        "- $H(T)$ is the impurity of the original node (e.g., calculated using Gini impurity).\n",
        "- $T_{\\text{left}}$ and $T_{\\text{right}}$ are the child nodes after the split.\n",
        "- $n_{\\text{left}}$ and $n_{\\text{right}}$ are the numbers of samples in the left and right child nodes, respectively.\n",
        "- $n$ is the total number of samples at the original node.\n",
        "\n"
      ],
      "metadata": {
        "id": "kqywMC9R3eg8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Majority Voting for Classification**\n",
        "\n",
        "For classification tasks, each tree in the forest makes a vote for the class:\n",
        "$$\\text{Final Class} = \\text{mode}(\\{\\text{class}_{\\text{tree}_1}, \\text{class}_{\\text{tree}_2}, \\ldots, \\text{class}_{\\text{tree}_N}\\})$$\n",
        "Where:\n",
        "- Each $\\text{tree}_i$ gives a class prediction, and the final class prediction is taken as the mode (most frequently occurring class) of these predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "ebkFdOdE3j8o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Averaging for Regression**\n",
        "\n",
        "For regression tasks, the prediction is the average of the predictions from all trees:\n",
        "$$\\text{Prediction} = \\frac{1}{N} \\sum_{i=1}^N \\text{prediction}_{\\text{tree}_i}$$\n",
        "Where:\n",
        "- $N$ is the number of trees.\n",
        "- $\\text{prediction}_{\\text{tree}_i}$ is the prediction made by the $i$-th tree.\n"
      ],
      "metadata": {
        "id": "L5zGPBaf3nSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Support vector machine (SVM)"
      ],
      "metadata": {
        "id": "YrwcyiOCvuB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM operates by finding the hyperplane that best separates the data into different classes. The goal is to maximize the margin between the data points of the classes, which can be intuitively thought of as trying to fit the widest possible \"street\" between the classes. The data points that define the margin are called support vectors, as they support or define the hyperplane."
      ],
      "metadata": {
        "id": "Cva_Vqug3vKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Mathematical Formulation of SVM"
      ],
      "metadata": {
        "id": "xZE_ifys3wub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**1. Linear SVM (Hard Margin)**\n",
        "For a linearly separable dataset with two classes, SVM finds a separating hyperplane with the form:\n",
        "$$w \\cdot x + b = 0$$\n",
        "Where:\n",
        "- $w$ is the normal vector to the hyperplane.\n",
        "- $x$ represents the feature vectors.\n",
        "- $b$ is the bias term.\n",
        "\n",
        "**Objective:** Maximize the margin between the classes subject to the constraint that all samples are classified correctly. This translates mathematically to:\n",
        "$$y_i (w \\cdot x_i + b) \\geq 1, \\forall i$$\n",
        "Where:\n",
        "- $y_i$ are the labels associated with each data point $x_i$, typically +1 or -1.\n",
        "To find $w$ and $b$, SVM solves the following optimization problem:\n",
        "$$\\min_{w, b} \\frac{1}{2} \\|w\\|^2$$\n",
        "Maximizing the margin (distance between the hyperplanes) is equivalent to minimizing $\\frac{1}{2} \\|w\\|^2$.\n",
        "\n"
      ],
      "metadata": {
        "id": "Kqhaolez4da9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**2. Linear SVM (Soft Margin)**\n",
        "In the real world, datasets are rarely perfectly linearly separable. To handle this, SVM introduces slack variables $\\xi_i$ to allow misclassification of difficult or noisy samples:\n",
        "$$y_i (w \\cdot x_i + b) \\geq 1 - \\xi_i, \\xi_i \\geq 0, \\forall i$$\n",
        "\n",
        "**Objective:**\n",
        "$$\\min_{w, b, \\xi} \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^n \\xi_i$$\n",
        "Where $C$ is a regularization parameter, balancing the trade-off between achieving a low error on the training data and minimizing the model complexity for better generalization.\n"
      ],
      "metadata": {
        "id": "yJWUV5w84i85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "####**3. Kernel Trick**\n",
        "For non-linear classification, SVM uses the kernel trick to transform the input space into a higher-dimensional space where a linear separator might exist:\n",
        "$$k(x_i, x_j) = \\phi(x_i) \\cdot \\phi(x_j)$$\n",
        "Where:\n",
        "- $k$ is the kernel function.\n",
        "- $\\phi$ is a function that maps input data into a higher-dimensional space.\n",
        "\n",
        "**Common kernels include:**\n",
        "- Linear: $$k(x_i, x_j) = x_i \\cdot x_j$$\n",
        "- Polynomial: $$k(x_i, x_j) = (\\gamma x_i \\cdot x_j + r)^d$$\n",
        "- Radial Basis Function (RBF): $$k(x_i, x_j) = \\exp(-\\gamma \\|x_i - x_j\\|^2)$$\n",
        "Where $\\gamma, r,$ and $d$ are parameters that can be adjusted.\n"
      ],
      "metadata": {
        "id": "Bo9c8cyT4lsN"
      }
    }
  ]
}